{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning PyTorch with Examples\n",
    "\n",
    "Fundamental concepts of PyTorch through self-contained examples.\n",
    "\n",
    "[Link to tutorial](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html)\n",
    "\n",
    "### Warm-up numpy\n",
    "\n",
    "Numpy provides an n-dimensional array object with many functions to manipulate them. While it doesn't perform any computation graphs, gradients etc right away, it is still very easy to implement a two-layer neural network by manually computing the forward and a backward pass through the network using numpy operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 488.68843952347845\n",
      "199 4.777699495716361\n",
      "299 0.07180600949248218\n",
      "399 0.0012233681498954392\n",
      "499 2.2656680249503788e-05\n"
     ]
    }
   ],
   "source": [
    "# N = batch_size; D_in = input dimensions\n",
    "# H = hidden dimensions; D_out = output dimensions\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# create random input and output data\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# Randomly init weights\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for t in range(500):\n",
    "    \n",
    "    # Forward Pass: compute predicted y\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "    \n",
    "    # Compute and print loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "    \n",
    "    # Backprop to compute gradients of w1 and w2 w.r.t loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "    \n",
    "    # Update weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch: Tensors\n",
    "\n",
    "PyTorch Tensors are conceptually identical to a numpy array but behind the scenes they can keep track of the computational graph and gradients. Also they can be run on a GPU to accelerate numeric computations.\n",
    "\n",
    "Running the above numpy computations with PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 437.33447265625\n",
      "199 1.3278000354766846\n",
      "299 0.006529013626277447\n",
      "399 0.00016863438941072673\n",
      "499 3.3541287848493084e-05\n"
     ]
    }
   ],
   "source": [
    "# init\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# random input and output data\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# random init weights\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for t in range(500):\n",
    "    \n",
    "    # Forward pass\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "    \n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "        \n",
    "    # Backpropagation\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "    \n",
    "    # Update weights using gradient descent\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd\n",
    "\n",
    "Autograd lets us compute automatic differentiation to automate the computation of backward passes in neural networks. \n",
    "\n",
    "When using autograd, the forward pass will create a computational graph where the nodes will be Tensors and the edges will be functions that produce the Tensors from input Tensors. Backpropagating through this graph then allows to easily compute gradients.\n",
    "\n",
    "Example: If *x* is a tensor that has *x.requires_grad=True*, then *x.grad* is another tensor that is holding the gradient of *x* w.r.t some scalar value.\n",
    "\n",
    "Implementing the above network with autograd to automate the backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 571.25732421875\n",
      "199 1.8490880727767944\n",
      "299 0.009281368926167488\n",
      "399 0.00019681017147377133\n",
      "499 3.391843711142428e-05\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# init\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# random input and output data\n",
    "# set requires_grad=False since we do not need to compute gradients for this\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# random tensors for weights\n",
    "# set requires_grad=True since we need to compute gradients for this during backward pass\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for t in range(500):\n",
    "    \n",
    "    # Forward pass: Compute predicted y\n",
    "    # Exactly the same as above\n",
    "    # But we no longer need to keep references to intermediate values since we \n",
    "    # are not implementing backward pass manually\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    \n",
    "    # Compute and print loss using operations on Tensors\n",
    "    # loss is a Tensor of shape (1,)\n",
    "    # loss.item() gets the scalar value\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "        \n",
    "    # using autograd to compute the backward pass\n",
    "    # This will compute the gradient of loss w.r.t all Tensors with requires_grad=True\n",
    "    # After this call w1.grad and w2.grad will be Tensors holding the gradient of the loss\n",
    "    # w.r.t w1 and w2 respectively\n",
    "    loss.backward()\n",
    "    \n",
    "    # Manually update weights using gradient descent\n",
    "    # Need to do this within torch.no_grad() as weights have requires_grad=True\n",
    "    # but we do not need to track this in autograd\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        \n",
    "        # Manually zero the gradients after updating weights\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining new autograd functions\n",
    "\n",
    "Each autograd operator is really just two functions that operate on Tensors.\n",
    "\n",
    "The **forward** function computes output Tensors from input Tensors. The **backward** function recieves the gradient of output Tensors w.r.t some scalar value and computes the gradient of input Tensors w.r.t that same scalar value.\n",
    "\n",
    "In PyTorch, we can definte our own autograd operator by defining a subclass of *torch.autograd.Function* and implementing the forward and backward functions. We can then use our new autograd operator by constructing an instance and calling it like a function, passing Tensors containing input data.\n",
    "\n",
    "Implementing the same two-layer network with custom autograd function for performing ReLU non-linearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myReLu(torch.autograd.Function):\n",
    "    \n",
    "    \"\"\"\n",
    "    Implementing custom autograd Functions by subclassing torch.autograd.Function\n",
    "    and implementing forward and backward passes which operate on Tensors.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In forward pass we receive a Tensor containing the input and return a Tensor\n",
    "        containing the output. \n",
    "        ctx is a context object that can be used to stash information for backward\n",
    "        computation. \n",
    "        Arbitrary objects can be cached for use in the backward pass using\n",
    "        ctx.save_for_backward_method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        w.r.t the output and we compute the gradient of the loss w.r.t the input.\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 336.20391845703125\n",
      "199 1.0574065446853638\n",
      "299 0.006670997478067875\n",
      "399 0.00020044234406668693\n",
      "499 3.946392462239601e-05\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# init\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# random input and output data\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# random init weights\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for t in range(500):\n",
    "    \n",
    "    # To apply function we use Function.apply method\n",
    "    relu = myReLu.apply\n",
    "    \n",
    "    # Forward pass: compute predicted y using operations;\n",
    "    # Computing ReLU using custom autograd operation.\n",
    "    y_pred = relu(x.mm(w1)).mm(w2)\n",
    "    \n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "        \n",
    "    # using aurograd to compute the backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update weights using gradient descent\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        \n",
    "        # Zero the gradients\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *nn* module\n",
    "\n",
    "### Pytorch:nn\n",
    "\n",
    "While computational graphs and autograd are very powerful for defining complex operators but for large neural networks raw autograd can be a bit too low-level.\n",
    "\n",
    "We frequently think of neural networks as arranging computation into layers, some of which have learnable parameters which will be optimized during training.\n",
    "\n",
    "The **Module** under **nn** package in PyTorch is roughly equivalent to neural network layers. It receives input Tensors and computes output Tensors, and also keeps hold of learnable parameters. It also defines a set of commonly used loss functions.\n",
    "\n",
    "Using **nn** package to implement the same two layer neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 2.678323745727539\n",
      "199 0.04020689055323601\n",
      "299 0.0009808404138311744\n",
      "399 2.8912812922499143e-05\n",
      "499 9.646383887229604e-07\n"
     ]
    }
   ],
   "source": [
    "# init\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# nn.Sequential contains other Modules, and applies them in sequence to produce output\n",
    "# Each Linear Module computes output from input using a linear function and holds\n",
    "# internal Tensors for its weights and bias\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out)\n",
    ")\n",
    "\n",
    "# It also has several loss functions implementations;\n",
    "# Here Mean Squared Error is used (MSE)\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-4\n",
    "\n",
    "for t in range(500):\n",
    "    \n",
    "    # Forward pass: Get y from X \n",
    "    # Overrides the __call__ operator so it can be used as functions\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    # Compute and print loss. \n",
    "    # We pass the predicted and true values of y\n",
    "    # Loss Function returns a Tensor containing the loss\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    \n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "        \n",
    "    # Zero the gradients before running the backward pass\n",
    "    model.zero_grad()\n",
    "    \n",
    "    # Backward pass: Compute the gradient of the loss w.r.t all the learnable params of model\n",
    "    # Internally, the params of each model are stored in Tensors with requires_grad=True\n",
    "    # So this will compute gradients for all learnable params in the model\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update the weights using gradient descent\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            \n",
    "            param -= learning_rate * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why do we need to *zero_grad()* the gradients before backward pass?\n",
    "\n",
    "- In PyTorch, we need to set the gradients to zero before starting to do backpropragation because PyTorch accumulates the gradients on subsequent backward passes. This is convenient while training RNNs. So, the default action is to accumulate (i.e. sum) the gradients on every loss.backward() call.\n",
    "\n",
    "- Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly. Else the gradient would point in some other direction than the intended direction towards the minimum (or maximum, in case of maximization objectives)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch: optim\n",
    "\n",
    "So far the weights of the models have been updated manually by mutating the Tensors holding learnable parameters *torch.no_grad()* (to avoid tracking history in autograd). This is not a huge burden for simple optimization algorithms like stochastic gradient descent, but in practice we often train neural networks using more sophisticated optimizers like AdaGrad, RMSProp, Adam, etc.\n",
    "\n",
    "The *optim* package provides implementations of several commonly used optimization algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 52.35138702392578\n",
      "199 0.7505016326904297\n",
      "299 0.007084321230649948\n",
      "399 0.0002512829378247261\n",
      "499 1.1613972674240358e-05\n"
     ]
    }
   ],
   "source": [
    "# init\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# nn.Sequential contains other Modules, and applies them in sequence to produce output\n",
    "# Each Linear Module computes output from input using a linear function and holds\n",
    "# internal Tensors for its weights and bias\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out)\n",
    ")\n",
    "\n",
    "# It also has several loss functions implementations;\n",
    "# Here Mean Squared Error is used (MSE)\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "# use the optim package to define an Optimizer that will manage the weights update.\n",
    "# Adam is used here\n",
    "# The first argument to the Adam constructor tells the optimizer\n",
    "# which Tensors should be updated\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for t in range(500):\n",
    "    \n",
    "    # Forward Pass : Compute y from X\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    \n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "        \n",
    "    # Before backward pass, use optimizer to zero all gradients for \n",
    "    # the variables it will update (which are the weights of the model)\n",
    "    # This is because gradients are accumulated in buffers (i.e. not overwritten)\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Backward Pass: Compute gradient of loss w.r.t model params\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update the parameters using step function of optimizer\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch: Custom nn Modules\n",
    "\n",
    "Sometimes you may need models that are more complex than existing Modules. For such cases custom nn modules can be defined by subclassing nn.Module and defining a *forward* function using other modules or autograd operations on Tensors.\n",
    "\n",
    "Implementing the above two-layer neural network as a custom Module subclass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them \n",
    "        as member variables.\n",
    "        \"\"\"\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        \"\"\"\n",
    "        Accept a Tensor of input data and return a Tensor of output data.\n",
    "        We can use Modules defined in the constructor as well as arbitrary \n",
    "        operators on Tensors\n",
    "        \"\"\"\n",
    "        h_relu = self.linear1(x).clamp(min=0)\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 1.62705397605896\n",
      "199 0.021302901208400726\n",
      "299 0.0011251665418967605\n",
      "399 9.295647032558918e-05\n",
      "499 8.531891580787487e-06\n"
     ]
    }
   ],
   "source": [
    "# init\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# random Tensors to hold inputs and output\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# construct model by instantiating class defined above\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "\n",
    "# Loss\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "# optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "\n",
    "for t in range(500):\n",
    "    \n",
    "    # Forward Pass\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    # Calc Loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    \n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "        \n",
    "    # zero grad\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # update weights\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch: Control Flow + Weight Sharing\n",
    "\n",
    "An example of dynamic graphs and weight sharing: A fully connected ReLU network that on each forward pass chooses a number between 1 and 4 and selects that number of hidden layers, reusing the same weights multiple times to compute the innermost hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class DynamicNet(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, D_in, H, D_out)\n",
    "        \"\"\"\n",
    "        Defining three nn.Linear instances that will be used in the forward pass\n",
    "        \"\"\"\n",
    "        super(DynamicNet, self).__init__()\n",
    "        self.input_linear = torch.nn.Linear(D_in, H)\n",
    "        self.middle_linear = torch.nn.Linear(H, H)\n",
    "        self.output_linear = torch.nn.Linear(H, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python-PyTorch",
   "language": "python",
   "name": "python-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
