{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning PyTorch with Examples\n",
    "\n",
    "Fundamental concepts of PyTorch through self-contained examples.\n",
    "\n",
    "[Link to tutorial](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html)\n",
    "\n",
    "### Warm-up numpy\n",
    "\n",
    "Numpy provides an n-dimensional array object with many functions to manipulate them. While it doesn't perform any computation graphs, gradients etc right away, it is still very easy to implement a two-layer neural network by manually computing the forward and a backward pass through the network using numpy operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 249.79337515387732\n",
      "199 0.642521286085513\n",
      "299 0.0026961331511322017\n",
      "399 1.3546705700769721e-05\n",
      "499 7.523998707837426e-08\n"
     ]
    }
   ],
   "source": [
    "# N = batch_size; D_in = input dimensions\n",
    "# H = hidden dimensions; D_out = output dimensions\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# create random input and output data\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# Randomly init weights\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for t in range(500):\n",
    "    \n",
    "    # Forward Pass: compute predicted y\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "    \n",
    "    # Compute and print loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "    \n",
    "    # Backprop to compute gradients of w1 and w2 w.r.t loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "    \n",
    "    # Update weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch: Tensors\n",
    "\n",
    "PyTorch Tensors are conceptually identical to a numpy array but behind the scenes they can keep track of the computational graph and gradients. Also they can be run on a GPU to accelerate numeric computations.\n",
    "\n",
    "Running the above numpy computations with PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 360.8602294921875\n",
      "199 1.2354717254638672\n",
      "299 0.010091866366565228\n",
      "399 0.0002943446743302047\n",
      "499 5.0072067097062245e-05\n"
     ]
    }
   ],
   "source": [
    "# init\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# random input and output data\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# random init weights\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for t in range(500):\n",
    "    \n",
    "    # Forward pass\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "    \n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "        \n",
    "    # Backpropagation\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "    \n",
    "    # Update weights using gradient descent\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd\n",
    "\n",
    "Autograd lets us compute automatic differentiation to automate the computation of backward passes in neural networks. \n",
    "\n",
    "When using autograd, the forward pass will create a computational graph where the nodes will be Tensors and the edges will be functions that produce the Tensors from input Tensors. Backpropagating through this graph then allows to easily compute gradients.\n",
    "\n",
    "Example: If *x* is a tensor that has *x.requires_grad=True*, then *x.grad* is another tensor that is holding the gradient of *x* w.r.t some scalar value.\n",
    "\n",
    "Implementing the above network with autograd to automate the backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 714.2745361328125\n",
      "199 4.589086055755615\n",
      "299 0.046332042664289474\n",
      "399 0.0008452735492028296\n",
      "499 9.571617556503043e-05\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# init\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# random input and output data\n",
    "# set requires_grad=False since we do not need to compute gradients for this\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# random tensors for weights\n",
    "# set requires_grad=True since we need to compute gradients for this during backward pass\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for t in range(500):\n",
    "    \n",
    "    # Forward pass: Compute predicted y\n",
    "    # Exactly the same as above\n",
    "    # But we no longer need to keep references to intermediate values since we \n",
    "    # are not implementing backward pass manually\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    \n",
    "    # Compute and print loss using operations on Tensors\n",
    "    # loss is a Tensor of shape (1,)\n",
    "    # loss.item() gets the scalar value\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "        \n",
    "    # using autograd to compute the backward pass\n",
    "    # This will compute the gradient of loss w.r.t all Tensors with requires_grad=True\n",
    "    # After this call w1.grad and w2.grad will be Tensors holding the gradient of the loss\n",
    "    # w.r.t w1 and w2 respectively\n",
    "    loss.backward()\n",
    "    \n",
    "    # Manually update weights using gradient descent\n",
    "    # Need to do this within torch.no_grad() as weights have requires_grad=True\n",
    "    # but we do not need to track this in autograd\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        \n",
    "        # Manually zero the gradients after updating weights\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining new autograd functions\n",
    "\n",
    "Each autograd operator is really just two functions that operate on Tensors.\n",
    "\n",
    "The **forward** function computes output Tensors from input Tensors. The **backward** function recieves the gradient of output Tensors w.r.t some scalar value and computes the gradient of input Tensors w.r.t that same scalar value.\n",
    "\n",
    "In PyTorch, we can definte our own autograd operator by defining a subclass of *torch.autograd.Function* and implementing the forward and backward functions. We can then use our new autograd operator by constructing an instance and calling it like a function, passing Tensors containing input data.\n",
    "\n",
    "Implementing the same two-layer network with custom autograd function for performing ReLU non-linearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myReLu(torch.autograd.Function):\n",
    "    \n",
    "    \"\"\"\n",
    "    Implementing custom autograd Functions by subclassing torch.autograd.Function\n",
    "    and implementing forward and backward passes which operate on Tensors.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In forward pass we receive a Tensor containing the input and return a Tensor\n",
    "        containing the output. \n",
    "        ctx is a context object that can be used to stash information for backward\n",
    "        computation. \n",
    "        Arbitrary objects can be cached for use in the backward pass using\n",
    "        ctx.save_for_backward_method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        w.r.t the output and we compute the gradient of the loss w.r.t the input.\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 1429.1990966796875\n",
      "199 25.401578903198242\n",
      "299 0.6302209496498108\n",
      "399 0.016869479790329933\n",
      "499 0.000762838521040976\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# init\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# random input and output data\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# random init weights\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for t in range(500):\n",
    "    \n",
    "    # To apply function we use Function.apply method\n",
    "    relu = myReLu.apply\n",
    "    \n",
    "    # Forward pass: compute predicted y using operations;\n",
    "    # Computing ReLU using custom autograd operation.\n",
    "    y_pred = relu(x.mm(w1)).mm(w2)\n",
    "    \n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "        \n",
    "    # using aurograd to compute the backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update weights using gradient descent\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        \n",
    "        # Zero the gradients\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *nn* module\n",
    "\n",
    "### Pytorch:nn\n",
    "\n",
    "While computational graphs and autograd are very powerful for defining complex operators but for large neural networks raw autograd can be a bit too low-level.\n",
    "\n",
    "We frequently think of neural networks as arranging computation into layers, some of which have learnable parameters which will be optimized during training.\n",
    "\n",
    "The **Module** under **nn** package in PyTorch is roughly equivalent to neural network layers. It receives input Tensors and computes output Tensors, and also keeps hold of learnable parameters. It also defines a set of commonly used loss functions.\n",
    "\n",
    "Using **nn** package to implement the same two layer neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# nn.Sequential contains other Modules, and applies them in sequence to produce output\n",
    "# Each Linear Module computes output from input using a linear function and holds\n",
    "# internal Tensors for its weights and bias\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python-PyTorch",
   "language": "python",
   "name": "python-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
