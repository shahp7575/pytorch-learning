{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ðŸ‘» Generate Sentences using vanilla recurrent neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import itertools # to perform operations on pythonic data structures\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import operator\n",
    "import sys # access to Python interpreter variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'book'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/names.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/words.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection book\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download nltk model data\n",
    "nltk.download('book')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 8000\n",
    "UNKNOWN_TOKEN = \"UNKNOWN_TOKEN\"\n",
    "SENTENCE_START_TOKEN = \"SENTENCE_START\"\n",
    "SENTENCE_END_TOKEN = \"SENTENCE_END\"\n",
    "\n",
    "CORPORA_DIR = \"/home/paperspace/nltk_data/corpora/state_union\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"PRESIDENT LYNDON B. JOHNSON'S ANNUAL MESSAGE TO THE CONGRESS ON THE STATE OF THE UNION January 17, 1968Mr.\",\n",
       " 'Speaker, Mr. President, Members of the Congress, and my fellow Americans:I was thinking as I was walking down the aisle tonight of what Sam Rayburn told me many years ago: The Congress always extends a very warm welcome to the President--as he comes in.Thank all of you very, very much.I have come once again to this Chamber--the home of our democracy--to give you, as the Constitution requires, \"Information of the State of the Union.',\n",
       " '\"I report to you that our country is challenged, at home and abroad:--that it is our will that is being tried, not our strength; our sense of purpose, not our ability to achieve a better America;--that we have the strength to meet our every challenge; the physical strength to hold the course of decency and compassion at home; and the moral strength to support the cause of peace in the world.',\n",
       " 'And I report to you that I believe, with abiding conviction, that this people--nurtured by their deep faith, tutored by their hard lessons, moved by their high aspirations--have the will to meet the trials that these times impose.',\n",
       " 'Since I reported to you last January:--Three elections have been held in Vietnam--in the midst of war and under the constant threat of violence.--A President, a Vice President, a House and Senate, and village officials have been chosen by popular, contested ballot.--The enemy has been defeated in battle after battle.--The number of South Vietnamese living in areas under Government protection tonight has grown by more than a million since January of last year.These are all marks of progress.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the dat and append SENTENCE_START and SENTENCE_END tokens\n",
    "print(\"Reading Data...\")\n",
    "\n",
    "# Read all file paths in CORPORA_DIR\n",
    "file_list = []\n",
    "\n",
    "for root, _, files in os.walk(CORPORA_DIR):\n",
    "    for filename in files:\n",
    "        file_list.append(os.path.join(root, filename))\n",
    "        \n",
    "sentences = []\n",
    "\n",
    "for files in file_list:\n",
    "    with open(files, 'r') as fin:\n",
    "        try:\n",
    "            str_form = fin.read().replace('\\n', '')\n",
    "            sentences.extend(nltk.sent_tokenize(str_form))\n",
    "        except UnicodeDecodeError:\n",
    "            # some sentences have weird characters. Ignore them\n",
    "            pass\n",
    "        \n",
    "# Get all sentenes in all files\n",
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"SENTENCE_START PRESIDENT LYNDON B. JOHNSON'S ANNUAL MESSAGE TO THE CONGRESS ON THE STATE OF THE UNION January 17, 1968Mr. SENTENCE_END\",\n",
       " 'SENTENCE_START Speaker, Mr. President, Members of the Congress, and my fellow Americans:I was thinking as I was walking down the aisle tonight of what Sam Rayburn told me many years ago: The Congress always extends a very warm welcome to the President--as he comes in.Thank all of you very, very much.I have come once again to this Chamber--the home of our democracy--to give you, as the Constitution requires, \"Information of the State of the Union. SENTENCE_END',\n",
       " 'SENTENCE_START \"I report to you that our country is challenged, at home and abroad:--that it is our will that is being tried, not our strength; our sense of purpose, not our ability to achieve a better America;--that we have the strength to meet our every challenge; the physical strength to hold the course of decency and compassion at home; and the moral strength to support the cause of peace in the world. SENTENCE_END',\n",
       " 'SENTENCE_START And I report to you that I believe, with abiding conviction, that this people--nurtured by their deep faith, tutored by their hard lessons, moved by their high aspirations--have the will to meet the trials that these times impose. SENTENCE_END',\n",
       " 'SENTENCE_START Since I reported to you last January:--Three elections have been held in Vietnam--in the midst of war and under the constant threat of violence.--A President, a Vice President, a House and Senate, and village officials have been chosen by popular, contested ballot.--The enemy has been defeated in battle after battle.--The number of South Vietnamese living in areas under Government protection tonight has grown by more than a million since January of last year.These are all marks of progress. SENTENCE_END']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add sentence delimiters\n",
    "# required to let RNN know what is the start and end of the sentence\n",
    "sentences = [SENTENCE_START_TOKEN + \" \" + x + \" \" + SENTENCE_END_TOKEN for x in sentences]\n",
    "\n",
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found  18331  unique word tokens.\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the sentences into words\n",
    "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "# Count the word frequencies\n",
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "\n",
    "print(\"Found \", len(word_freq.items()), \" unique word tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since it may take forever train, it is trained only on 8000 most frequent words\n",
    "# Get the most common words and build index_to_word and word_to_index vectors\n",
    "vocab = word_freq.most_common(VOCAB_SIZE-1) \n",
    "index_to_word = [x[0] for x in vocab] # extract word\n",
    "index_to_word.append(UNKNOWN_TOKEN)\n",
    "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)]) # Create word-index map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Vocabulary size  8000\n",
      "The most frequent word is ' the ' and appeared  17514  times\n",
      "The least frequent word is ' diminishing ' and appeared  2  times\n"
     ]
    }
   ],
   "source": [
    "print(\"Using Vocabulary size \", VOCAB_SIZE)\n",
    "print(\"The most frequent word is '\", vocab[0][0], \"' and appeared \", vocab[0][1], \" times\")\n",
    "print(\"The least frequent word is '\", vocab[-1][0], \"' and appeared \", vocab[-1][1], \" times\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all words not in our vocab with the UNKNOWN_TOKEN\n",
    "for i, sent in enumerate(tokenized_sentences):\n",
    "    tokenized_sentences[i] = [w if w in word_to_index else UNKNOWN_TOKEN for w in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/anaconda3/envs/pytorch/lib/python3.8/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "# Create training data\n",
    "# Every X represents a word. Every y represents a word that follows it in the sequence\n",
    "X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences])\n",
    "y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([687, 3421, 3422, 3423, 785, 1634, 1572, 1058, 228, 723, 776, 228, 816, 404, 228, 817, 622, 2066, 1, 7999, 4, 3]),\n",
       "       list([594, 1, 488, 130, 1, 491, 5, 0, 50, 1, 7, 111, 432, 76, 62, 15, 80, 2413, 29, 15, 80, 4707, 212, 0, 2721, 146, 5, 106, 3140, 4175, 1573, 153, 107, 57, 247, 62, 38, 50, 328, 4176, 9, 184, 5515, 974, 6, 0, 130, 26, 29, 220, 1219, 7999, 30, 5, 37, 184, 1, 184, 7999, 17, 186, 434, 233, 6, 20, 818, 26, 0, 169, 5, 10, 417, 26, 6, 162, 37, 1, 29, 0, 933, 512, 1, 124, 3760, 5, 0, 248, 5, 0, 215, 4, 3]),\n",
       "       list([124, 15, 540, 6, 37, 12, 10, 86, 14, 3141, 1, 46, 169, 7, 525, 62, 26, 12, 25, 14, 10, 16, 12, 14, 312, 1440, 1, 21, 10, 175, 54, 10, 724, 5, 379, 1, 21, 10, 713, 6, 369, 9, 165, 44, 54, 26, 12, 11, 17, 0, 175, 6, 195, 10, 65, 283, 54, 0, 2067, 175, 6, 548, 0, 402, 5, 2295, 7, 1220, 46, 169, 54, 7, 0, 954, 175, 6, 141, 0, 441, 5, 81, 8, 0, 40, 4, 3]),\n",
       "       ..., list([168, 17, 61, 5, 0, 2669, 26, 21, 5, 0, 755, 4, 3]),\n",
       "       list([42, 25, 14, 167, 566, 12, 0, 100, 2665, 5, 0, 1033, 5, 30, 203, 1, 2056, 657, 8, 0, 3281, 5, 0, 3663, 1, 35, 61, 0, 6744, 24, 49, 11, 17, 1440, 7, 19, 1096, 6, 7584, 10, 402, 4, 3]),\n",
       "       list([324, 189, 29, 11, 136, 24, 66, 2746, 1, 103, 16, 18, 566, 198, 8, 259, 943, 1, 196, 317, 431, 7, 317, 72, 23, 1586, 11, 7999, 369, 9, 33, 2123, 7999, 6, 25, 1, 15, 956, 1, 14, 9, 1559, 4222, 29, 11, 195, 151, 8, 58, 138, 448, 5, 20, 1, 0, 138, 1041, 5, 0, 6520, 50, 4, 3])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([2, 687, 3421, 3422, 3423, 785, 1634, 1572, 1058, 228, 723, 776, 228, 816, 404, 228, 817, 622, 2066, 1, 7999, 4]),\n",
       "       list([2, 594, 1, 488, 130, 1, 491, 5, 0, 50, 1, 7, 111, 432, 76, 62, 15, 80, 2413, 29, 15, 80, 4707, 212, 0, 2721, 146, 5, 106, 3140, 4175, 1573, 153, 107, 57, 247, 62, 38, 50, 328, 4176, 9, 184, 5515, 974, 6, 0, 130, 26, 29, 220, 1219, 7999, 30, 5, 37, 184, 1, 184, 7999, 17, 186, 434, 233, 6, 20, 818, 26, 0, 169, 5, 10, 417, 26, 6, 162, 37, 1, 29, 0, 933, 512, 1, 124, 3760, 5, 0, 248, 5, 0, 215, 4]),\n",
       "       list([2, 124, 15, 540, 6, 37, 12, 10, 86, 14, 3141, 1, 46, 169, 7, 525, 62, 26, 12, 25, 14, 10, 16, 12, 14, 312, 1440, 1, 21, 10, 175, 54, 10, 724, 5, 379, 1, 21, 10, 713, 6, 369, 9, 165, 44, 54, 26, 12, 11, 17, 0, 175, 6, 195, 10, 65, 283, 54, 0, 2067, 175, 6, 548, 0, 402, 5, 2295, 7, 1220, 46, 169, 54, 7, 0, 954, 175, 6, 141, 0, 441, 5, 81, 8, 0, 40, 4]),\n",
       "       ..., list([2, 168, 17, 61, 5, 0, 2669, 26, 21, 5, 0, 755, 4]),\n",
       "       list([2, 42, 25, 14, 167, 566, 12, 0, 100, 2665, 5, 0, 1033, 5, 30, 203, 1, 2056, 657, 8, 0, 3281, 5, 0, 3663, 1, 35, 61, 0, 6744, 24, 49, 11, 17, 1440, 7, 19, 1096, 6, 7584, 10, 402, 4]),\n",
       "       list([2, 324, 189, 29, 11, 136, 24, 66, 2746, 1, 103, 16, 18, 566, 198, 8, 259, 943, 1, 196, 317, 431, 7, 317, 72, 23, 1586, 11, 7999, 369, 9, 33, 2123, 7999, 6, 25, 1, 15, 956, 1, 14, 9, 1559, 4222, 29, 11, 195, 151, 8, 58, 138, 448, 5, 20, 1, 0, 138, 1041, 5, 0, 6520, 50, 4])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python-PyTorch",
   "language": "python",
   "name": "python-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
