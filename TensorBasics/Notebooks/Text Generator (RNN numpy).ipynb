{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ðŸ‘» Generate Sentences using vanilla recurrent neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import itertools # to perform operations on pythonic data structures\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import operator\n",
    "import sys # access to Python interpreter variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download nltk model data (do it just once)\n",
    "# nltk.download('book')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 8000\n",
    "UNKNOWN_TOKEN = \"UNKNOWN_TOKEN\"\n",
    "SENTENCE_START_TOKEN = \"SENTENCE_START\"\n",
    "SENTENCE_END_TOKEN = \"SENTENCE_END\"\n",
    "\n",
    "CORPORA_DIR = \"/home/paperspace/nltk_data/corpora/state_union\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"PRESIDENT LYNDON B. JOHNSON'S ANNUAL MESSAGE TO THE CONGRESS ON THE STATE OF THE UNION January 17, 1968Mr.\",\n",
       " 'Speaker, Mr. President, Members of the Congress, and my fellow Americans:I was thinking as I was walking down the aisle tonight of what Sam Rayburn told me many years ago: The Congress always extends a very warm welcome to the President--as he comes in.Thank all of you very, very much.I have come once again to this Chamber--the home of our democracy--to give you, as the Constitution requires, \"Information of the State of the Union.',\n",
       " '\"I report to you that our country is challenged, at home and abroad:--that it is our will that is being tried, not our strength; our sense of purpose, not our ability to achieve a better America;--that we have the strength to meet our every challenge; the physical strength to hold the course of decency and compassion at home; and the moral strength to support the cause of peace in the world.',\n",
       " 'And I report to you that I believe, with abiding conviction, that this people--nurtured by their deep faith, tutored by their hard lessons, moved by their high aspirations--have the will to meet the trials that these times impose.',\n",
       " 'Since I reported to you last January:--Three elections have been held in Vietnam--in the midst of war and under the constant threat of violence.--A President, a Vice President, a House and Senate, and village officials have been chosen by popular, contested ballot.--The enemy has been defeated in battle after battle.--The number of South Vietnamese living in areas under Government protection tonight has grown by more than a million since January of last year.These are all marks of progress.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the dat and append SENTENCE_START and SENTENCE_END tokens\n",
    "print(\"Reading Data...\")\n",
    "\n",
    "# Read all file paths in CORPORA_DIR\n",
    "file_list = []\n",
    "\n",
    "for root, _, files in os.walk(CORPORA_DIR):\n",
    "    for filename in files:\n",
    "        file_list.append(os.path.join(root, filename))\n",
    "        \n",
    "sentences = []\n",
    "\n",
    "for files in file_list:\n",
    "    with open(files, 'r') as fin:\n",
    "        try:\n",
    "            str_form = fin.read().replace('\\n', '')\n",
    "            sentences.extend(nltk.sent_tokenize(str_form))\n",
    "        except UnicodeDecodeError:\n",
    "            # some sentences have weird characters. Ignore them\n",
    "            pass\n",
    "        \n",
    "# Get all sentenes in all files\n",
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"SENTENCE_START PRESIDENT LYNDON B. JOHNSON'S ANNUAL MESSAGE TO THE CONGRESS ON THE STATE OF THE UNION January 17, 1968Mr. SENTENCE_END\",\n",
       " 'SENTENCE_START Speaker, Mr. President, Members of the Congress, and my fellow Americans:I was thinking as I was walking down the aisle tonight of what Sam Rayburn told me many years ago: The Congress always extends a very warm welcome to the President--as he comes in.Thank all of you very, very much.I have come once again to this Chamber--the home of our democracy--to give you, as the Constitution requires, \"Information of the State of the Union. SENTENCE_END',\n",
       " 'SENTENCE_START \"I report to you that our country is challenged, at home and abroad:--that it is our will that is being tried, not our strength; our sense of purpose, not our ability to achieve a better America;--that we have the strength to meet our every challenge; the physical strength to hold the course of decency and compassion at home; and the moral strength to support the cause of peace in the world. SENTENCE_END',\n",
       " 'SENTENCE_START And I report to you that I believe, with abiding conviction, that this people--nurtured by their deep faith, tutored by their hard lessons, moved by their high aspirations--have the will to meet the trials that these times impose. SENTENCE_END',\n",
       " 'SENTENCE_START Since I reported to you last January:--Three elections have been held in Vietnam--in the midst of war and under the constant threat of violence.--A President, a Vice President, a House and Senate, and village officials have been chosen by popular, contested ballot.--The enemy has been defeated in battle after battle.--The number of South Vietnamese living in areas under Government protection tonight has grown by more than a million since January of last year.These are all marks of progress. SENTENCE_END']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add sentence delimiters\n",
    "# required to let RNN know what is the start and end of the sentence\n",
    "sentences = [SENTENCE_START_TOKEN + \" \" + x + \" \" + SENTENCE_END_TOKEN for x in sentences]\n",
    "\n",
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found  18331  unique word tokens.\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the sentences into words\n",
    "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "# Count the word frequencies\n",
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "\n",
    "print(\"Found \", len(word_freq.items()), \" unique word tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since it may take forever train, it is trained only on 8000 most frequent words\n",
    "# Get the most common words and build index_to_word and word_to_index vectors\n",
    "vocab = word_freq.most_common(VOCAB_SIZE-1) \n",
    "index_to_word = [x[0] for x in vocab] # extract word\n",
    "index_to_word.append(UNKNOWN_TOKEN)\n",
    "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)]) # Create word-index map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Vocabulary size  8000\n",
      "The most frequent word is ' the ' and appeared  17514  times\n",
      "The least frequent word is ' diminishing ' and appeared  2  times\n"
     ]
    }
   ],
   "source": [
    "print(\"Using Vocabulary size \", VOCAB_SIZE)\n",
    "print(\"The most frequent word is '\", vocab[0][0], \"' and appeared \", vocab[0][1], \" times\")\n",
    "print(\"The least frequent word is '\", vocab[-1][0], \"' and appeared \", vocab[-1][1], \" times\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all words not in our vocab with the UNKNOWN_TOKEN\n",
    "for i, sent in enumerate(tokenized_sentences):\n",
    "    tokenized_sentences[i] = [w if w in word_to_index else UNKNOWN_TOKEN for w in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/anaconda3/envs/pytorch/lib/python3.8/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "# Create training data\n",
    "# Every X represents a word. Every y represents a word that follows it in the sequence\n",
    "X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences])\n",
    "y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([687, 3421, 3422, 3423, 785, 1634, 1572, 1058, 228, 723, 776, 228, 816, 404, 228, 817, 622, 2066, 1, 7999, 4, 3]),\n",
       "       list([594, 1, 488, 130, 1, 491, 5, 0, 50, 1, 7, 111, 432, 76, 62, 15, 80, 2413, 29, 15, 80, 4707, 212, 0, 2721, 146, 5, 106, 3140, 4175, 1573, 153, 107, 57, 247, 62, 38, 50, 328, 4176, 9, 184, 5515, 974, 6, 0, 130, 26, 29, 220, 1219, 7999, 30, 5, 37, 184, 1, 184, 7999, 17, 186, 434, 233, 6, 20, 818, 26, 0, 169, 5, 10, 417, 26, 6, 162, 37, 1, 29, 0, 933, 512, 1, 124, 3760, 5, 0, 248, 5, 0, 215, 4, 3]),\n",
       "       list([124, 15, 540, 6, 37, 12, 10, 86, 14, 3141, 1, 46, 169, 7, 525, 62, 26, 12, 25, 14, 10, 16, 12, 14, 312, 1440, 1, 21, 10, 175, 54, 10, 724, 5, 379, 1, 21, 10, 713, 6, 369, 9, 165, 44, 54, 26, 12, 11, 17, 0, 175, 6, 195, 10, 65, 283, 54, 0, 2067, 175, 6, 548, 0, 402, 5, 2295, 7, 1220, 46, 169, 54, 7, 0, 954, 175, 6, 141, 0, 441, 5, 81, 8, 0, 40, 4, 3]),\n",
       "       ..., list([168, 17, 61, 5, 0, 2669, 26, 21, 5, 0, 755, 4, 3]),\n",
       "       list([42, 25, 14, 167, 566, 12, 0, 100, 2665, 5, 0, 1033, 5, 30, 203, 1, 2056, 657, 8, 0, 3281, 5, 0, 3663, 1, 35, 61, 0, 6744, 24, 49, 11, 17, 1440, 7, 19, 1096, 6, 7584, 10, 402, 4, 3]),\n",
       "       list([324, 189, 29, 11, 136, 24, 66, 2746, 1, 103, 16, 18, 566, 198, 8, 259, 943, 1, 196, 317, 431, 7, 317, 72, 23, 1586, 11, 7999, 369, 9, 33, 2123, 7999, 6, 25, 1, 15, 956, 1, 14, 9, 1559, 4222, 29, 11, 195, 151, 8, 58, 138, 448, 5, 20, 1, 0, 138, 1041, 5, 0, 6520, 50, 4, 3])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([2, 687, 3421, 3422, 3423, 785, 1634, 1572, 1058, 228, 723, 776, 228, 816, 404, 228, 817, 622, 2066, 1, 7999, 4]),\n",
       "       list([2, 594, 1, 488, 130, 1, 491, 5, 0, 50, 1, 7, 111, 432, 76, 62, 15, 80, 2413, 29, 15, 80, 4707, 212, 0, 2721, 146, 5, 106, 3140, 4175, 1573, 153, 107, 57, 247, 62, 38, 50, 328, 4176, 9, 184, 5515, 974, 6, 0, 130, 26, 29, 220, 1219, 7999, 30, 5, 37, 184, 1, 184, 7999, 17, 186, 434, 233, 6, 20, 818, 26, 0, 169, 5, 10, 417, 26, 6, 162, 37, 1, 29, 0, 933, 512, 1, 124, 3760, 5, 0, 248, 5, 0, 215, 4]),\n",
       "       list([2, 124, 15, 540, 6, 37, 12, 10, 86, 14, 3141, 1, 46, 169, 7, 525, 62, 26, 12, 25, 14, 10, 16, 12, 14, 312, 1440, 1, 21, 10, 175, 54, 10, 724, 5, 379, 1, 21, 10, 713, 6, 369, 9, 165, 44, 54, 26, 12, 11, 17, 0, 175, 6, 195, 10, 65, 283, 54, 0, 2067, 175, 6, 548, 0, 402, 5, 2295, 7, 1220, 46, 169, 54, 7, 0, 954, 175, 6, 141, 0, 441, 5, 81, 8, 0, 40, 4]),\n",
       "       ..., list([2, 168, 17, 61, 5, 0, 2669, 26, 21, 5, 0, 755, 4]),\n",
       "       list([2, 42, 25, 14, 167, 566, 12, 0, 100, 2665, 5, 0, 1033, 5, 30, 203, 1, 2056, 657, 8, 0, 3281, 5, 0, 3663, 1, 35, 61, 0, 6744, 24, 49, 11, 17, 1440, 7, 19, 1096, 6, 7584, 10, 402, 4]),\n",
       "       list([2, 324, 189, 29, 11, 136, 24, 66, 2746, 1, 103, 16, 18, 566, 198, 8, 259, 943, 1, 196, 317, 431, 7, 317, 72, 23, 1586, 11, 7999, 369, 9, 33, 2123, 7999, 6, 25, 1, 15, 956, 1, 14, 9, 1559, 4222, 29, 11, 195, 151, 8, 58, 138, 448, 5, 20, 1, 0, 138, 1041, 5, 0, 6520, 50, 4])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input is a word and the Output is the predicted next word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('SENTENCE_START', 'There'), ('There', 'is'), ('is', 'no'), ('no', 'mystery'), ('mystery', 'about'), ('about', 'the'), ('the', 'questions'), ('questions', 'which'), ('which', 'must'), ('must', 'be'), ('be', 'answered'), ('answered', 'before'), ('before', 'the'), ('the', 'bombing'), ('bombing', 'is'), ('is', 'stopped.We'), ('stopped.We', 'believe'), ('believe', 'that'), ('that', 'any'), ('any', 'talks'), ('talks', 'should'), ('should', 'follow'), ('follow', 'the'), ('the', 'San'), ('San', 'Antonio'), ('Antonio', 'formula'), ('formula', 'that'), ('that', 'I'), ('I', 'stated'), ('stated', 'last'), ('last', 'September'), ('September', ','), (',', 'which'), ('which', 'said'), ('said', ':'), (':', '--'), ('--', 'The'), ('The', 'bombing'), ('bombing', 'would'), ('would', 'stop'), ('stop', 'immediately'), ('immediately', 'if'), ('if', 'talks'), ('talks', 'would'), ('would', 'take'), ('take', 'place'), ('place', 'promptly'), ('promptly', 'and'), ('and', 'with'), ('with', 'reasonable'), ('reasonable', 'hopes'), ('hopes', 'that'), ('that', 'they'), ('they', 'would'), ('would', 'be'), ('be', 'UNKNOWN_TOKEN'), ('UNKNOWN_TOKEN', '--'), ('--', 'And'), ('And', 'the'), ('the', 'other'), ('other', 'side'), ('side', 'must'), ('must', 'not'), ('not', 'take'), ('take', 'advantage'), ('advantage', 'of'), ('of', 'our'), ('our', 'restraint'), ('restraint', 'as'), ('as', 'they'), ('they', 'have'), ('have', 'in'), ('in', 'the'), ('the', 'past'), ('past', '.'), ('.', 'SENTENCE_END')]\n"
     ]
    }
   ],
   "source": [
    "# a training data example\n",
    "x_example, y_example = X_train[10], y_train[10]\n",
    "# print(list(zip(x_example, y_example)))\n",
    "\n",
    "print(list(zip([index_to_word[x] for x in x_example], [index_to_word[y] for y in y_example])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax function\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each set sof scores in x.\"\"\"\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN\n",
    "class RNN:\n",
    "    \n",
    "    def __init__(self, word_dim, hidden_dim=50, bptt_truncate=4):\n",
    "        \n",
    "        # Assign instance variables\n",
    "        self.word_dim = word_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        \n",
    "        # Randomly initialize the network parameters\n",
    "        self.U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))\n",
    "        \n",
    "    def forward_propagation(self, x):\n",
    "        # The total number of time steps\n",
    "        T = len(x)\n",
    "        # During forward propagation we save all hiden states in s because we need them later\n",
    "        # We add one additional element for the initial hidden, which we set to 0\n",
    "        s = np.zeros(T + 1, self.hidden_dim)\n",
    "        s[-1] = np.zeros(self.hidden_dim)\n",
    "        \n",
    "        # The outputs at each time step. We save them for later.\n",
    "        o = np.zeros((T, self.word_dim))\n",
    "        \n",
    "        # For each time step...\n",
    "        for t in np.arange(T):\n",
    "            # indexing U by x[t]. This is the same as multiplying U with a one-hot vector\n",
    "            s[t] = np.tanh(self.U[:, x[t]] + self.W.dot(s[t-1]))\n",
    "            o[t] = softmax(self.V.dot(s[t]))\n",
    "            \n",
    "        return [o, s]\n",
    "    \n",
    "    def predict(self, x):\n",
    "        # Perform forward propagation and return the index of the highest score\n",
    "        o, s = self.forward_propagation(x)\n",
    "        return np.argmax(o, axis=1)\n",
    "    \n",
    "    def calculate_total_loss(self, x, y):\n",
    "        L = 0\n",
    "        # For each sentence\n",
    "        for i in np.arange(len(y)):\n",
    "            o, s = self.forward_propagation(x[i])\n",
    "            \n",
    "            correct_word_predictions = o[np.arange(len(y[i])), y[i]]\n",
    "            L += -1 * np.sum(np.log(correct_word_predictions))\n",
    "            \n",
    "        return L\n",
    "    \n",
    "    def calculate_loss(self, x, y):\n",
    "        # Divide the total loss by the number of training examples\n",
    "        N = np.sum((len(y_i) for y_i in y))\n",
    "        return self.calculate_total_loss(x,y)/N\n",
    "    \n",
    "    def sgd_step(self, x, y, learning_rate):\n",
    "        \n",
    "        # gradients\n",
    "        dLdU, dLdV, dLdW = self.bptt(x, y)\n",
    "        \n",
    "        # Change parameters according to gradients and learning rate\n",
    "        self.U -= learning_rate * dLdU\n",
    "        self.V -= learning_rate * dLdV\n",
    "        self.W -= learning_rate * dLdW\n",
    "        \n",
    "    def bptt(self, x, y):\n",
    "        T = len(y)\n",
    "        # peform forward propagation\n",
    "        o, s = self.forward_propagation(x)\n",
    "        # accumualate gradients in these variables\n",
    "        dLdU = np.zeros(self.U.shape)\n",
    "        dLdV = np.zeros(self.V.shape)\n",
    "        dLdW = np.zeros(self.W.shape)\n",
    "        delta_o = o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python-PyTorch",
   "language": "python",
   "name": "python-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
